{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All data used in this notebook comes from Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_feature_importance(feature_importance, features):\n",
    "    feature_imp = pd.DataFrame(zip(feature_importance, features), columns=['Value','Feature'])\n",
    "    plt.figure(figsize=(20, 500))\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data = feature_imp.sort_values(by = \"Value\", ascending = False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_features(feature_importance, feature):\n",
    "    feature_imp = pd.DataFrame(zip(feature_importance, features), columns=['Value','Feature'])\n",
    "    feature_imp = feature_imp.sort_values(by = 'Value', ascending = False)\n",
    "    return feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Bayesian Optimization to detect the best parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(reduce_train, reduce_test, useful_features, n_splits, num_leaves, max_depth, min_child_weight, feature_fraction, lambda_l1, lambda_l2, \n",
    "          bagging_fraction, min_data_in_leaf, learning_rate, reg_alpha, reg_lambda, n_estimators):\n",
    "     \n",
    "        params =  {'num_leaves': int(num_leaves),  \n",
    "            'max_depth' : int(max_depth),\n",
    "           'min_child_weight': min_child_weight,\n",
    "           'feature_fraction': feature_fraction,\n",
    "           'bagging_fraction': bagging_fraction,\n",
    "           'min_data_in_leaf': int(min_data_in_leaf), \n",
    "           'objective': 'regression',\n",
    "           \"metric\": 'rmse',\n",
    "           'learning_rate': learning_rate, \n",
    "           \"boosting_type\": \"gbdt\",\n",
    "           \"bagging_seed\": 11,\n",
    "           \"verbosity\": -1,\n",
    "           'reg_alpha': reg_alpha,\n",
    "           'reg_lambda': reg_lambda,\n",
    "           'random_state': 46,\n",
    "           'num_threads': 16,\n",
    "           'lambda_l1': lambda_l1,  \n",
    "           'lambda_l2': lambda_l2, \n",
    "           'n_estimators': int(n_estimators),\n",
    "           'early_stopping': 150\n",
    "    }\n",
    "        def run_lgb(reduce_train, reduce_test, useful_features, n_splits = n_splits):\n",
    "            #useful_features.remove('installation_id')\n",
    "            rmse_score_list = []\n",
    "            useful_features = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in useful_features]\n",
    "            reduce_train = change_json(reduce_train)\n",
    "            reduce_test = change_json(reduce_test)\n",
    "            kf = StratifiedKFold(n_splits = n_splits, random_state = 42, shuffle = True)\n",
    "            oof_predict = np.zeros((len(reduce_train), ))\n",
    "            y_pred = np.zeros((len(reduce_test), ))\n",
    "            for fold, (train_index, test_index) in enumerate(kf.split(reduce_train, reduce_train[target])):\n",
    "                X_train = reduce_train[useful_features].iloc[train_index]\n",
    "                X_val = reduce_train[useful_features].iloc[test_index]\n",
    "                y_train = reduce_train[target].iloc[train_index]\n",
    "                y_val = reduce_train[target].iloc[test_index]\n",
    "                train_set = lgb.Dataset(X_train, y_train, categorical_feature = categoricals)\n",
    "                val_set = lgb.Dataset(X_val, y_val, categorical_feature = categoricals)\n",
    "                lgb_model = lgb.train(params, train_set, num_boost_round = params['n_estimators'], valid_sets = [train_set, val_set],\n",
    "                             early_stopping_rounds = params['early_stopping'])\n",
    "                val_predict = lgb_model.predict(X_val)\n",
    "                rmse_score = np.sqrt(mean_squared_error(val_predict, y_val))\n",
    "                rmse_score_list.append(rmse_score)\n",
    "            return -np.mean(rmse_score_list)\n",
    "        \n",
    "        return run_lgb(reduce_train, reduce_test, useful_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "partial_model = partial(model, reduce_train, reduce_test, new_features, n_splits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_LGB = {\n",
    "    'num_leaves' : (50, 100),\n",
    "    'max_depth': (8, 30),\n",
    "    'min_child_weight' : (0.01, 0.6),\n",
    "    'min_data_in_leaf' : (80, 120),\n",
    "    'feature_fraction' : (0.1, 0.8),\n",
    "    'lambda_l1': (0, 10),\n",
    "    'lambda_l2': (0, 10),\n",
    "    'bagging_fraction': (0.2, 1),\n",
    "    'learning_rate': (0.01, 0.8),\n",
    "    'reg_alpha' : (0.1 , 5), \n",
    "    'reg_lambda' : (0.1, 5),\n",
    "    'n_estimators' : (5000,8000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points = 16\n",
    "n_iter = 16\n",
    "LGB_BO = BayesianOptimization(partial_model, bounds_LGB, random_state=1029)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points = init_points, n_iter = n_iter, acq='ei', alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_LGB_BO_params = LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the selected parameters to cross validation and find the best features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_params =  {'num_leaves': 50,  \n",
    "            'max_depth' : 30,\n",
    "           'min_child_weight': 0.01,\n",
    "           'feature_fraction': 0.8,\n",
    "           'bagging_fraction': 0.2,\n",
    "           'min_data_in_leaf': 80, \n",
    "           'objective': 'regression',\n",
    "           \"metric\": 'rmse',\n",
    "           'learning_rate': 0.01, \n",
    "           \"boosting_type\": \"gbdt\",\n",
    "           \"bagging_seed\": 11,\n",
    "           \"verbosity\": -1,\n",
    "           'reg_alpha': 50,\n",
    "           'reg_lambda': 0.1,\n",
    "           'random_state': 46,\n",
    "           'num_threads': 16,\n",
    "           'lambda_l1': 10,  \n",
    "           'lambda_l2': 0, \n",
    "           'n_estimators': 5149,\n",
    "           'early_stopping': 150\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bayes, oof_predict_bayes, feature_importance_bayes = run_lgb_regression(reduce_train, reduce_test, new_features, 5, 10, bayesian_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_predict_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_qwk_lgb_regr(reduce_train[target], oof_predict_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_fold_1 = get_important_features(feature_importance_bayes['fold_1'], new_features)\n",
    "feature_imp_fold_2 = get_important_features(feature_importance_bayes['fold_2'], new_features)\n",
    "feature_imp_fold_3 = get_important_features(feature_importance_bayes['fold_3'], new_features)\n",
    "feature_imp_fold_4 = get_important_features(feature_importance_bayes['fold_4'], new_features)\n",
    "feature_imp_fold_5 = get_important_features(feature_importance_bayes['fold_5'], new_features)\n",
    "feature_imp_fold_list = [feature_imp_fold_1, feature_imp_fold_2, feature_imp_fold_3, feature_imp_fold_4, feature_imp_fold_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_feature_imp(feature_imp_fold_list):\n",
    "    feature_imp_fold_1 = feature_imp_fold_list[0].set_index('Feature')\n",
    "    feature_imp_fold_2 = feature_imp_fold_list[1].set_index('Feature')\n",
    "    feature_imp_fold_3 = feature_imp_fold_list[2].set_index('Feature')\n",
    "    feature_imp_fold_4 = feature_imp_fold_list[3].set_index('Feature')\n",
    "    feature_imp_fold_5 = feature_imp_fold_list[4].set_index('Feature')\n",
    "    df1 = pd.merge(feature_imp_fold_1, feature_imp_fold_2, how = 'inner', left_index = True, right_index = True)\n",
    "    df2 = df1.merge(feature_imp_fold_3, how = 'inner', left_index = True, right_index = True)\n",
    "    df3 = df2.merge(feature_imp_fold_4, how = 'inner', left_index = True, right_index = True)\n",
    "    final_df = df3.merge(feature_imp_fold_5, how = 'inner', left_index = True, right_index = True)\n",
    "    final_df.columns = ['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']\n",
    "    final_df['average'] = final_df[['fold_1', 'fold_2', 'fold_3', 'fold_4', 'fold_5']].mean(axis = 1)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_from_all_folders = merge_feature_imp(feature_imp_fold_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_from_all_folders = feature_importance_from_all_folders.sort_values('average', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = feature_importance_from_all_folders.loc[(feature_importance_from_all_folders['average'] > 5) & (feature_importance_from_all_folders['average'] < 600), :].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = list(top_features.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'session_title' not in top_features:\n",
    "    top_features.append('session_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the top features, we run lightgbm and tune the parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_model_top_features = partial(model, reduce_train, reduce_test, top_features, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points = 16\n",
    "n_iter = 16\n",
    "LGB_BO_top_features = BayesianOptimization(partial_model_top_features, bounds_LGB, random_state = 1029)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO_top_features.maximize(init_points = init_points, n_iter = n_iter, acq='ei', alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_LGB_BO_top_features_params = LGB_BO_top_features.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_BO_top_features.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_LGB_BO_top_features_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bayes_top_features, oof_predict_bayes_top_features, feature_importance_bayes_top_features = run_lgb_regression(reduce_train, reduce_test, \n",
    "                                                                                             top_features, 5, 10, bayesian_params_top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_predict_bayes_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_qwk_lgb_regr(reduce_train[target], oof_predict_bayes_top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_to_cat(y_regress):\n",
    "    dist = Counter(reduce_train['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(reduce_train)\n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_regress, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_regress)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg_to_cat(y_pred_bayes_top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['accuracy_group'] = y_pred.astype(int)\n",
    "sample_submission.to_csv('./submission.csv', index=False)\n",
    "sample_submission['accuracy_group'].value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
